distributed: True
image_to_wandb: True
display_freq: 3000
print_freq: 500
save_latest_freq: 1000
save_epoch_freq: 10
max_epoch: 1000


data:
    name: cad
    type: data.cad_dataset::CadDataset
    preprocess_mode: resize_and_crop
    path: /datasets/msha/cad
    num_workers: 3
    sub_path: 256-256
    resolution: 128
    scale_param: 0.05
    maxframe: 100
    sub_frame: 5
    test_fold: 1
    train:
      batch_size: 3          # real_batch_size: 4 * 2 (source-->target & target --> source) * 2 (GPUs) = 16
      distributed: False
    val:
      batch_size: 2
      distributed: False

model:
    type: models.unet_autoenc::BeatGANsAutoencModel
    param:
        image_size: 128
        in_channels: 1  # 3+20, assuming this is a sum
        model_channels: 128
        out_channels: 2  # 3*2, also learns sigma
        num_res_blocks: 2
        num_input_res_blocks: null  # Assuming None should be null in YAML
        embed_channels: 512
        attention_resolutions:
          - 32
          - 16
          - 8
        time_embed_channels: null
        dropout: 0.1
        channel_mult:
          - 1
          - 1
          - 2
          - 2
          - 4
          - 4
        input_channel_mult: null
        conv_resample: true
        dims: 3
        num_classes: null
        use_checkpoint: false
        num_heads: 1
        num_head_channels: -1
        num_heads_upsample: -1
        resblock_updown: true
        use_new_attention_order: false
        resnet_two_cond: false
        resnet_cond_channels: null
        resnet_use_zero_module: true
        attn_checkpoint: false
        attn_numheads: 4
        enc_out_channels: 512
        enc_attn_resolutions: null
        enc_pool: "adaptivenonzero"  # Strings with special characters might need quotes
        enc_num_res_block: 2
        enc_channel_mult:
          - 1
          - 1
          - 2
          - 2
          - 4
          - 4
          - 4
        enc_grad_checkpoint: false
        latent_net_conf: null

trainer:
    type: trainer.diffusion_trainer::DiffusionTrainer
    start_iteration: 0
    loss_weight:
      weight_mean: 2
      weight_vb: 1.5


optimizer:
    type: adam
    lr: 2e-5

scheduler:
    type: step
    lr: 2e-5
    step_size: 1000000000
    gamma: 1
    n_iter: 2400000
    warmup: 5000
    decay: [linear, flat]

diffusion:
    guidance_prob: 0.1
    sample_algorithm: ddim
    cond_scale: 2
    beta_schedule:
        schedule: linear
        n_timestep: 1000
        linear_start: 1e-4
        linear_end: 2e-2