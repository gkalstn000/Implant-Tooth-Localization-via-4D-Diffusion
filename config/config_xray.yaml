distributed: True
image_to_wandb: True
display_freq: 5000
print_freq: 100
save_latest_freq: 2000
save_regular_freq: 15000
save_epoch_freq: 10000
max_epoch: 12000


data:
    name: xray
    type: data.cad_dataset::CadDataset
    preprocess_mode: resize_and_crop
    path: /datasets/msha/cad
    num_workers: 3
    resolution: 128
    scale_param: 0.05
    maxframe: 16
    sub_frame: 4
    test_fold: 3
    train:
      batch_size: 6          # real_batch_size: 4 * 2 (source-->target & target --> source) * 2 (GPUs) = 16
      distributed: True
    val:
      batch_size: 2
      distributed: True

model:
    type: models.unet::BeatGANsUNetModel
    param:
        image_size: 128
        in_channels: 1  # 3+20, assuming this is a sum
        model_channels: 64
        out_channels: 2  # 3*2, also learns sigma
        num_res_blocks: 2
        num_input_res_blocks: null  # Assuming None should be null in YAML
        embed_channels: 512
        attention_resolutions:
          - 32
          - 16
          - 8
        time_embed_channels: null
        dropout: 0.1
        channel_mult:
          - 1
          - 2
          - 2
          - 4
        input_channel_mult: null
        conv_resample: true
        dims: 3
        num_classes: null
        use_checkpoint: false
        num_heads: 1
        num_head_channels: -1
        num_heads_upsample: -1
        resblock_updown: true
        use_new_attention_order: false
        resnet_two_cond: false
        resnet_cond_channels: null
        resnet_use_zero_module: true
        attn_checkpoint: false
        attn_numheads: 4
        enc_out_channels: 512
        enc_attn_resolutions: null
        enc_pool: "adaptivenonzero"  # Strings with special characters might need quotes
        enc_num_res_block: 2
        enc_channel_mult:
          - 1
          - 1
          - 2
          - 2
          - 4
          - 4
          - 4
        enc_grad_checkpoint: false
        latent_net_conf: null

trainer:
    type: trainer.xray_trainer::Trainer
    start_iteration: 0
    loss_weight:
      weight_mean: 2
      weight_vb: 1.5


optimizer:
    type: adam
    lr: 2e-5

scheduler:
    type: step
    lr: 2e-5
    step_size: 1000000000
    gamma: 1
    n_iter: 2400000
    warmup: 5000
    decay: [linear, flat]

diffusion:
    guidance_prob: 0.1
    sample_algorithm: ddim #ddpm
    cond_scale: 2
    beta_schedule:
        schedule: linear
        n_timestep: 1000
        linear_start: 1e-4
        linear_end: 2e-2